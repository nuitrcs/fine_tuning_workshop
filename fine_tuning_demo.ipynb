{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IY9AaWuVDuK"
      },
      "source": [
        "# Demo for the Fine-Tuning LLMs for Text Analysis Workshop\n",
        "\n",
        "Note: the code in this notebook is for demonstration purposes. You need to adapt it for a research project. The hyperparameters used here are also not optimal.\n",
        "\n",
        "## Install libraries required for the workshop and that are not available by default in Google Colab\n",
        "\n",
        "This chunk of code checks whether you're working on Google Colab. If so, it installs `evaluate`. Otherwise (e.g., you're working on your local machine), it doesn't do anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGi7lTeSUz75"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    ! pip install evaluate\n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik3z9rg9VVOX"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "*This is related to step 2 in the slide \"Checklist for fine-tuning a model\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkzDzDh6VWF5"
      },
      "outputs": [],
      "source": [
        "# To work with dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# To work with arrays\n",
        "import numpy as np\n",
        "\n",
        "# To split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# To prepare the data in the expected format\n",
        "from datasets import DatasetDict, Dataset\n",
        "\n",
        "# To fine-tune model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# To evaluate model\n",
        "import evaluate\n",
        "\n",
        "# To select the computing device and other uses of PyTorch\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybP0VneVZEU"
      },
      "source": [
        "## Check whether we're using CPU or GPU\n",
        "\n",
        "CUDA is a parallel computing platform and API developed by NVIDIA to use their GPUs. While CUDA is for NVIDIA's GPUs, MPS is for Apple Silicon. In the code chunk below, if it says \"cuda\" or \"mps\", it means we're using a GPU.\n",
        "\n",
        "*This is related to step 2 in the slide \"Checklist for fine-tuning a model\".*\n",
        "\n",
        "(You can find more information about CPUs vs. GPUs in the slide \"CPUs vs. GPUs\" in the appendix slides.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFN-1_KpVbLf"
      },
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcfJwJbGVvn0"
      },
      "source": [
        "## Read the data\n",
        "\n",
        "*This is related to step 3 in the slide \"Checklist for fine-tuning a model\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwGVJEI6VwhD"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/MoritzLaurer/less-annotating-with-bert-nli/refs/heads/master/data_clean/df_manifesto_protectionism_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/MoritzLaurer/less-annotating-with-bert-nli/refs/heads/master/data_clean/df_manifesto_protectionism_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se0qr_rkV6CB"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDzfpTKJVyJr"
      },
      "outputs": [],
      "source": [
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq_aJO3AV_Tw"
      },
      "outputs": [],
      "source": [
        "train['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_H7B9sUWIT0"
      },
      "outputs": [],
      "source": [
        "train['label_text'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMk0RGmWDFa"
      },
      "source": [
        "Here we're going to focus on negative vs. positive for simplicity and speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV7uB7LWV7eN"
      },
      "outputs": [],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psi4r0ixV3Zw"
      },
      "outputs": [],
      "source": [
        "test.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqbN2hmzV5QI"
      },
      "outputs": [],
      "source": [
        "test['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_brGP3rWQEZ"
      },
      "outputs": [],
      "source": [
        "test['label_text'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44_JA1wLWWd7"
      },
      "source": [
        "## Clean data\n",
        "\n",
        "Drop \"Other\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTAcLst4WRse"
      },
      "outputs": [],
      "source": [
        "train = train[train['label'] != 0]\n",
        "test = test[test['label'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jz5pAw7WcNb"
      },
      "outputs": [],
      "source": [
        "train['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hdsxk5fWeKi"
      },
      "outputs": [],
      "source": [
        "test['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv-lMZzmWj8A"
      },
      "source": [
        "Label 1 as 0 and 2 as 1. This is important because otherwise later, in `trainer.train()`, you'll get an error (`CUDA error: device-side assert triggered`) because `CrossEntropyLoss`, which is used by default in `Trainer` for classificaton, expects class labels starting at 0 (see [here](https://stackoverflow.com/questions/51691563/cuda-runtime-error-59-device-side-assert-triggered) and [here](https://drdroid.io/stack-diagnosis/pytorch-runtimeerror--cuda-error--device-side-assert-triggered))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgySgJvxWfNz"
      },
      "outputs": [],
      "source": [
        "train['label'] = train['label'] - 1\n",
        "test['label'] = test['label'] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4l_9DnrWrqT"
      },
      "outputs": [],
      "source": [
        "train['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnLuZlPdWuHz"
      },
      "outputs": [],
      "source": [
        "test['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgfN0fZ0W0Hn"
      },
      "source": [
        "Put together all the text (preceding, original, and following):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vxl-bh-Wv0V"
      },
      "outputs": [],
      "source": [
        "train['text'] = train['text_preceding'] + \" \" + train['text_original'] + \" \" + train['text_following']\n",
        "test['text'] = test['text_preceding'] + \" \" + test['text_original'] + \" \" + test['text_following']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVeFZg4MW4oh"
      },
      "outputs": [],
      "source": [
        "train[['text_preceding', 'text_original', 'text_following', 'text']].head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSs1iMMzW9JV"
      },
      "outputs": [],
      "source": [
        "test[['text_preceding', 'text_original', 'text_following', 'text']].head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4XxqCuaXD3b"
      },
      "source": [
        "Remove missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cEVFflwXCfQ"
      },
      "outputs": [],
      "source": [
        "train = train[train['text'].notna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkC9Oi_WZzzY"
      },
      "source": [
        "Here's what we have as a reminder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QskSTcNpZ2lx"
      },
      "outputs": [],
      "source": [
        "print(train.loc[1058, 'label_text'])\n",
        "print(train.loc[1058, 'label'])\n",
        "print(train.loc[1058, 'text'])\n",
        "print(\"\")\n",
        "print(train.loc[2111, 'label_text'])\n",
        "print(train.loc[2111, 'label'])\n",
        "print(train.loc[2111, 'text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25xN5e5dza6"
      },
      "source": [
        "\"1\" or \"Protectionism: Positive\" means that the text is in favor of protectionism. \"0\" or \"Protectionism: Negative\" means that the text is against protectionism.\n",
        "\n",
        "## Split `test` into `validation`, `test`, and `new`\n",
        "\n",
        "*This is related to step 3 in the slide \"Checklist for fine-tuning a model\".*\n",
        "\n",
        "(You can find more information about the train/validation/test split approach in the slide \"Train/validation/test split​\" in the appendix slides.)\n",
        "\n",
        "**`new` is only for demonstration purposes for this workshop.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTqzcTudypi"
      },
      "outputs": [],
      "source": [
        "SEED = 6325\n",
        "\n",
        "validation, temp = train_test_split(test, test_size=0.5, random_state=SEED)\n",
        "\n",
        "test, new = train_test_split(temp, test_size=0.5, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci1t-ZOBeqhx"
      },
      "outputs": [],
      "source": [
        "validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6AnEdqme5AJ"
      },
      "outputs": [],
      "source": [
        "validation['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8uiRdg5eqkU"
      },
      "outputs": [],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCIUz9Xee5gt"
      },
      "outputs": [],
      "source": [
        "test['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-madFDMIeqmq"
      },
      "outputs": [],
      "source": [
        "new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D99yoUJne5-A"
      },
      "outputs": [],
      "source": [
        "new['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCwYE2lWZp1t"
      },
      "source": [
        "## Convert to [`DatasetDict`](https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/main_classes#datasets.DatasetDict)\n",
        "\n",
        "This prepares the data in the expected format for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIiKxPvNZslH"
      },
      "outputs": [],
      "source": [
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train[['label', 'text']].reset_index(drop=True)),\n",
        "    'validation': Dataset.from_pandas(validation[['label', 'text']].reset_index(drop=True)),\n",
        "    'test': Dataset.from_pandas(test[['label', 'text']].reset_index(drop=True))\n",
        "})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KKb1mRnafCN"
      },
      "source": [
        "## Set name of pretrained model that we're going to use\n",
        "\n",
        "*This is related to step 4 in the slide \"Checklist for fine-tuning a model\".*\n",
        "\n",
        "(You can find more information about different types of models in the section \"NLP\" of the appendix slides.)\n",
        "\n",
        "Name from the [Hugging Face website](https://huggingface.co/distilbert/distilbert-base-uncased):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5mnlq0iZ8mE"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"distilbert/distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ_liuh1atI8"
      },
      "source": [
        "`MODEL_NAME` now is the name of the **PRETRAINED MODEL**.\n",
        "\n",
        "## Tokenize the data\n",
        "\n",
        "Load appropriate tokenizer for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJD76OrhakbU"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy0tzV9dbCQp"
      },
      "source": [
        "Create function to tokenize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdm9EuiVbBqW"
      },
      "outputs": [],
      "source": [
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        # We added max_length for speed.\n",
        "        # Otherwise it defaults to the max the model can take https://huggingface.co/docs/transformers/en/pad_truncation\n",
        "        max_length=128\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWEXSzo3bR_I"
      },
      "outputs": [],
      "source": [
        "example_input = {\"text\": [\"I love this workshop!\"]}\n",
        "\n",
        "tokenize(example_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MysFSHuhbRh1"
      },
      "source": [
        "The output is:\n",
        "- `input_ids`: a list of token IDs corresponding to the input texts. `101` is the CLS token, `1045` is for \"I\", ..., `102` is SEP, and the rest is padding.\n",
        "- `attention_mask` is a list indicating which tokens should be attended to, where 1 is a real token and 0 is padding.\n",
        "\n",
        "(You can find more information about text embeddings in the slide \"Text embeddings (ML review for fine-tuning)​\" in the appendix slides.)\n",
        "\n",
        "Let's tokenize the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2LyZDvZcIxw"
      },
      "outputs": [],
      "source": [
        "# batched = TRUE to operate on batches of examples rather than individual for speed\n",
        "# https://huggingface.co/docs/datasets/en/process#batch-processing\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCD1wsRldSg_"
      },
      "source": [
        "## Specify how many number of categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdAYn7bCcoOi"
      },
      "outputs": [],
      "source": [
        "NUM_LABELS = len(train['label'].unique())\n",
        "NUM_LABELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaGL7qSVdZgy"
      },
      "source": [
        "## Load the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2VgTQgVdXA5"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0B-fJARdg-l"
      },
      "source": [
        "`model` is now the **PRETRAINED MODEL**.\n",
        "\n",
        "## Define metrics for evaluation\n",
        "\n",
        "*This is related to step 5 in the slide \"Checklist for fine-tuning a model\".*\n",
        "\n",
        "(You can find more information about performance metrics in the \"Performance metrics\" section of the appendix slides.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRQ1qU9Xdij1"
      },
      "outputs": [],
      "source": [
        "metrics = evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"f1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyacYvpVd-fz"
      },
      "source": [
        "Create function to calculate metrics. We need to create `compute_metrics_closure` because we want `metrics` to be an argument but `Trainer` expects the function to accept only one argument (an instance of `EvalPrediction`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FDYns9neDdn"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_closure(metrics):\n",
        "  \"\"\"\n",
        "  Creates a compute_metrics function with the provided evaluation metrics.\n",
        "\n",
        "  Args:\n",
        "      metrics (evaluate.EvaluationModule): A combined metric object from the `evaluate` library.\n",
        "\n",
        "  Returns:\n",
        "      function: A function that computes the provided metrics for given model predictions.\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_metrics(eval_pred):\n",
        "      \"\"\"\n",
        "      Computes evaluation metrics for model predictions.\n",
        "\n",
        "      Args:\n",
        "          eval_pred (tuple): A tuple containing:\n",
        "              - logits (np.ndarray): The raw output predictions from the model. (Logits, also called logg-odds, are equal to the logarithm of the odds, which are the probability divided by one minus the probability.)\n",
        "              - labels (np.ndarray): The true labels corresponding to the inputs.\n",
        "      Returns:\n",
        "          dict: A dictionary containing the computed metric(s).\n",
        "      \"\"\"\n",
        "\n",
        "      # Unpack the logits and labels from the evaluation prediction tuple\n",
        "      logits, labels = eval_pred\n",
        "\n",
        "      # Convert the raw logits to predicted class labels by selecting the index with the highest logit value\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "      # Compute and return the evaluation metric(s) using the predictions and true labels\n",
        "      return metrics.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  return compute_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG0fYSNDfErK"
      },
      "source": [
        "## Define arguments for fine-tuning\n",
        "\n",
        "*This is related to step 6 in the slide \"Checklist for fine-tuning a model\".*\n",
        "\n",
        "(You can find more information about some of the hyperparameters of neural networks in the \"Neural networks\" section of the appendix slides.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZCcdLYBfGPy"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/transformers/v4.7.0/main_classes/trainer.html#transformers.TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    # The output directory where the model predictions and checkpoints will be written.\n",
        "    output_dir=\"./output\",\n",
        "    # Total number of training epochs to perform.\n",
        "    num_train_epochs=1,\n",
        "    # The batch size per GPU core/CPU for training.\n",
        "    per_device_train_batch_size=8,\n",
        "    # The batch size per GPU core/CPU for evaluation.\n",
        "    per_device_eval_batch_size=8,\n",
        "    # Learning rate\n",
        "    learning_rate=5e-5,\n",
        "    # Weight decay\n",
        "    weight_decay=0.0,\n",
        "    # The logging strategy to adopt during training. Here, logging is done at the end of each epoch.\n",
        "    logging_strategy=\"epoch\",\n",
        "    # The list of integrations to report the results and logs to.\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-oqeWOHf_Oa"
      },
      "source": [
        "## Fine-tune the pretrained model\n",
        "\n",
        "*This is related to step 6 in the slide \"Checklist for fine-tuning a model\".*\n",
        "\n",
        "(You can find more information on training a neural network in the \"Neural networks\" section of the appendix slides.)\n",
        "\n",
        "Define [`Trainer`](https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.Trainer) object. This object takes a pretrained model and prepares it for training and evaluation (remember that `model` is the **PRETRAINED MODEL**). It abstracts a lot of the complexity that would be necessary using PyTorch directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7B5Mu68gAGe"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics_closure(metrics)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2HOWvUFhoJ3"
      },
      "source": [
        "`trainer` is a training manager object, which includes the **FINE-TUNED MODEL**, the dataset, the training arguments, and the performance metrics.\n",
        "\n",
        "Fine-tune model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-DHIq4Tho3Z"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYCOCs68gioD"
      },
      "source": [
        "- `global_step`: total number of optimization steps (batches) processed.\n",
        "- `training_loss`: average loss computed over all batches in this training run.\n",
        "- `metrics`: dictionary with additional stats.\n",
        "  - `train_runtime`: time taken for training.\n",
        "  - `train_samples_per_second`: how many samples processed per second.\n",
        "  - `train_steps_per_second`: steps performed per second.\n",
        "  - `total_flos`: total floating-point operations used.\n",
        "  - `train_loss`: average loss.\n",
        "  - `epoch`: epoch number completed.\n",
        "\n",
        "## Test the fine-tuned model\n",
        "\n",
        "*This is related to step 7 in the slide \"Checklist for fine-tuning a model\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfPWpGzxgi6z"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(eval_dataset=dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0XgoU0glqQ"
      },
      "source": [
        "- `eval_loss`: average loss calculated over the evaluation (test) dataset.\n",
        "- `eval_accuracy`, `eval_precision`, `eval_recall`, `eval_f1`: metrics computed via the `compute_metrics` function.\n",
        "- `eval_runtime`: time taken to run the evaluation loop.\n",
        "- `eval_samples_per_second`: how many test samples were processed per second.\n",
        "- `eval_steps_per_second`: rate at which evaluation batches were processed.\n",
        "- `epoch`: epoch number at which evaluation was done.\n",
        "\n",
        "## Generate predictions on new data with the fine-tuned model\n",
        "\n",
        "Create dataset for `new` data since this is the format that `Trainer` expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUdWFvargnjr"
      },
      "outputs": [],
      "source": [
        "new_dataset = Dataset.from_pandas(new[['text']].reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUbEamkYo-g3"
      },
      "source": [
        "Tokenize new data with the same tokenizer used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdzSs-ZcnqyZ"
      },
      "outputs": [],
      "source": [
        "new_dataset = new_dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmrX7nCypBYY"
      },
      "source": [
        "Use model to get predictions -- logits (logits, also called logg-odds, are equal to the logarithm of the odds, which are the probability divided by one minus the probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo7MBG0Tny9f"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(new_dataset)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvBqI2LpqdXp"
      },
      "outputs": [],
      "source": [
        "predictions.predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4wZJWBbpFyo"
      },
      "source": [
        "Get predicted classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2p5Q0oan5Xx"
      },
      "outputs": [],
      "source": [
        "# https://numpy.org/doc/2.2/reference/generated/numpy.argmax.html\n",
        "# Returns the index of the maximum value along an axis.\n",
        "# Axis=1 to find the index of the maximum value in each row of the 2D array.\n",
        "# In this case, that's the class with the highest logit, i.e., the predicted class.\n",
        "predicted_classes = np.argmax(predictions.predictions, axis=1)\n",
        "predicted_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1Pqd3cpKDb"
      },
      "source": [
        "Add predictions to the original `new` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5C6-OrpoECM"
      },
      "outputs": [],
      "source": [
        "new['predicted_class'] = predicted_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSHff8ovoOTr"
      },
      "outputs": [],
      "source": [
        "new.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDJlWDgYoeQY"
      },
      "outputs": [],
      "source": [
        "new['predicted_class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvXstSsKpUdf"
      },
      "source": [
        "Since our \"`new`\" data actually had labels, we can compare the predictions with the labels, **which wouldn't be the case in an actual research project.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i38Be9CohMU"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(new['label'], new['predicted_class'], margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the examples that the model got wrong. Remember that \"1\" or \"Protectionism: Positive\" means that the text is in favor of protectionism. \"0\" or \"Protectionism: Negative\" means that the text is against protectionism."
      ],
      "metadata": {
        "id": "ETOfMWY4ECGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = new[new['label'] != new['predicted_class']][['label', 'label_text', 'predicted_class', 'text']]\n",
        "\n",
        "for i, row in errors.iterrows():\n",
        "  print(f\"Correct label: {row['label']}\")\n",
        "  print(f\"What the correct label means: {row['label_text']}\")\n",
        "  print(f\"Predicted label: {row['predicted_class']}\")\n",
        "  print(f\"Text: {row['text']}\")\n",
        "  print(\"=\" * 100)"
      ],
      "metadata": {
        "id": "bRD0rzKdEAfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRNX719Syotk"
      },
      "source": [
        "## Answer research question\n",
        "\n",
        "Remember the research question: in which country is protectionist political speech more prevalent?​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MkcfYm_0jwQ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(new['country_name'], new['predicted_class'], normalize='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OC5x-YJ1zrr"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(new['country_name'], new['predicted_class'], margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH_U35-biTWa"
      },
      "source": [
        "## Continue learning\n",
        "\n",
        "This notebook includes a lot of URLs that you can go to to learn more about the code. You can also consult these websites:\n",
        "- https://huggingface.co/docs/transformers/en/training\n",
        "- https://medium.com/@hassaanidrees7/fine-tuning-transformers-techniques-for-improving-model-performance-4b4353e8ba93\n",
        "- https://huggingface.co/learn/llm-course/chapter3/3\n",
        "\n",
        "This notebook uses code produced by ChatGPT. That's okay to do as long as you consider the privacy, security, and intellectual property implications, as well as understand the code. We do have a [workshop on writing effective prompts for coding with LLMs](https://github.com/nuitrcs/promptEngineering)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}